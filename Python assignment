# import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv("/kaggle/input/machine-dataset/ai4i2020.csv")

# Create a new column 'interaction_feature' by multiplying air temperature and process temperature
df['interaction_feature'] = df['Air temperature [K]'] * df['Process temperature [K]']


QUESTION 1 :
Do any outliers exist in the tool wear column, and should they be removed or adjusted?

##A
# Plot histogram
plt.figure(figsize=(10, 6))
sns.histplot(df['interaction_feature'], bins=10, kde=True, color='skyblue')
plt.xlabel('Interaction Feature')
plt.ylabel('Frequency')
plt.title('Histogram of Interaction Feature')
plt.grid(True)
plt.show()

QUESTION 2 : Can we create a new feature that combines air temperature and process temperature to capture their interaction effect?

##B
# Check for outliers in the tool wear column
sns.set_style("whitegrid")
plt.figure(figsize=(8, 6))
sns.violinplot(y=df['Tool wear [min]'], color='skyblue')
plt.title('Violin Plot of Tool Wear')
plt.ylabel('Tool Wear [min]')
plt.grid(True)
plt.show()


QUESTION 3:  How can we encode categorical variables such as product quality variant (L, M, H) for use in linear regression?

##C
# One-Hot Encoding
one_hot_encoded = pd.get_dummies(df['Type'], prefix='Type')
df_encoded_one_hot = pd.concat([df, one_hot_encoded], axis=1)

# Bar plot for One-Hot Encoded Data
plt.figure(figsize=(8, 6))
df_encoded_one_hot[['Type_L', 'Type_M', 'Type_H']].sum().plot(kind='bar', color=['skyblue', 'orange', 'green'])
plt.xlabel('Type')
plt.ylabel('Frequency')
plt.title('Frequency Distribution of Types (One-Hot Encoded)')
plt.xticks(rotation=0)
plt.grid(axis='y')
plt.show()

# Ordinal Encoding
ordinal_mapping = {'L': 0, 'M': 1, 'H': 2}
df['Type_encoded'] = df['Type'].map(ordinal_mapping)

plt.figure(figsize=(10, 6))
sns.histplot(df['Type_encoded'], bins=3, kde=True, color='skyblue')
plt.xlabel('Ordinal Encoded Type')
plt.ylabel('Frequency')
plt.title('Histogram of Ordinal Encoded Type')
plt.grid(True)
plt.show()


QUESTION 4: Can we engineer new features based on domain knowledge, such as the product of torque and rotational speed?


##D
# Scatter plot of Torque [Nm] vs. Rotational speed [rpm]
plt.figure(figsize=(12, 8))
plt.scatter(df['Torque [Nm]'], df['Rotational speed [rpm]'], c=df['interaction_feature'], cmap='viridis', s=100, alpha=0.8)
plt.xlabel('Torque [Nm]')
plt.ylabel('Rotational Speed [rpm]')
plt.title('Scatter Plot with Torque-Rotational Speed Interaction')
plt.colorbar(label='Interaction Feature')
plt.grid(True)
plt.show()


QUESTION 5:  Is there any duplicated data present in the dataset, and if so, how should it be handled to avoid bias in the model?


##E

# Assuming you have a DataFrame named 'data'
# Example DataFrame creation

# Check for duplicated rows
duplicates = df[df.duplicated()]

# Display duplicated rows
print("Duplicated Rows:")
print(duplicates)

# Remove duplicates
clean_df = df.drop_duplicates()

# Display cleaned DataFrame
print("\nDataFrame after removing duplicates:")
print(clean_df)


# Plotting
plt.figure(figsize=(8, 6))
plt.bar(["Duplicated", "Unique"], [num_duplicates, num_total_rows - num_duplicates], color=['red', 'blue'])
plt.title("Duplicated vs. Unique Rows")
plt.xlabel("Type of Rows")
plt.ylabel("Number of Rows")
plt.show()


QUESTION 6:Should we normalize the numerical features to ensure they have similar scales for linear regression?
##F
from sklearn.preprocessing import StandardScaler

# Separate numerical and categorical features
numerical_features = df_cleaned.select_dtypes(include=['float64', 'int64'])

# Normalize numerical features
scaler = StandardScaler()
normalized_features = scaler.fit_transform(numerical_features)

# Convert normalized_features back to DataFrame
normalized_df = pd.DataFrame(normalized_features, columns=numerical_features.columns)

# Display the first few rows of the normalized DataFrame
print(normalized_df.head())

plt.figure(figsize=(10, 6))
sns.boxplot(data=normalized_df, orient='h', palette='Set2')
plt.title('Boxplot of Normalized Numerical Features')
plt.show()

# Concatenate the normalized features with categorical features
df_normalized = pd.concat([normalized_df, df_cleaned.drop(numerical_features.columns, axis=1)], axis=1)

# Display the first few rows of the normalized DataFrame
print(df_normalized.head())

QUESTION 7: Apply feature selection techniques to identify the most relevant features for predicting tool wear with linear regression

from sklearn.model_selection import train_test_split

# Create a new DataFrame with the top 5 features and target variable
X_new = df_cleaned_encoded.loc[:, top_features]
y = df_cleaned['Tool wear [min]']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=0.2, random_state=42)

# Initialize the linear regression model
lr = LinearRegression()

# Train the model on the training data
lr.fit(X_train, y_train)

# Make predictions on the testing data
y_pred = lr.predict(X_test)

# Create a pairplot of the test set with the predicted tool wear
p = sns.pairplot(X_test, height=2.5, aspect=1.5)

# Add the predicted tool wear as a diagonal plot
p.diag_axes[0].hist(y_test, bins=20, color='red', alpha=0.5, label='Actual Tool Wear')
p.diag_axes[0].hist(y_pred, bins=20, color='blue', alpha=0.5, label='Predicted Tool Wear')

# Add grid
plt.grid(True)

# Show the plot
plt.show()


QUESTION 8: Which metrics can be applied to check correct prediction of tool wear using linear regression?

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Check the column names to identify the correct column name for product quality variant
print(df.columns)

# Check the data types of each column
print(df.dtypes)

# Drop non-numeric columns (assuming 'product id' is non-numeric)
df_numeric = df.drop(columns=['Product ID'])

# Perform one-hot encoding for 'product quality'
df_encoded = pd.get_dummies(df_numeric, columns=['Type'])



# Split the dataset into features (X) and target variable (y)
X = df_encoded.drop(columns=['Tool wear [min]'])  # Features
y = df_encoded['Tool wear [min]']  # Target variable

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the linear regression model
lr = LinearRegression()

# Train the model
lr.fit(X_train, y_train)

# Make predictions
y_pred_train = lr.predict(X_train)
y_pred_test = lr.predict(X_test)

# Evaluate the model
train_rmse = mean_squared_error(y_train, y_pred_train, squared=False)
test_rmse = mean_squared_error(y_test, y_pred_test, squared=False)
train_r2 = r2_score(y_train, y_pred_train)
test_r2 = r2_score(y_test, y_pred_test)

print("Train RMSE:", train_rmse)
print("Test RMSE:", test_rmse)
print("Train R^2 Score:", train_r2)
print("Test R^2 Score:", test_r2)


# Plotting actual vs predicted values for training set and testing set together
plt.figure(figsize=(10, 6))

# Plot actual vs predicted values for training set
plt.scatter(y_train, y_pred_train, color='blue', label='Training Set')

# Plot actual vs predicted values for testing set
plt.scatter(y_test, y_pred_test, color='red', label='Testing Set')

# Plot the diagonal line
plt.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=2)

# Add labels and title
plt.xlabel('Actual Tool Wear [min]')
plt.ylabel('Predicted Tool Wear [min]')
plt.title('Actual vs Predicted Tool Wear')

# Add legend and grid
plt.legend()
plt.grid(True)

# Show the plot
plt.show()
